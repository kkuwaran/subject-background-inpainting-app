{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import SamModel, SamProcessor\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/sam-vit-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SamModel.from_pretrained(model_name).to(device)\n",
    "processor = SamProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rgb(mask):\n",
    "    \"\"\"\n",
    "    Transforms a binary mask into an RGBA image for visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    bg_transparent = np.zeros(mask.shape + (4, ), dtype=np.uint8)\n",
    "    \n",
    "    # Color the area we will replace in green\n",
    "    # (this vector is [Red, Green, Blue, Alpha])\n",
    "    bg_transparent[mask == 1] = [0, 255, 0, 127]\n",
    "    \n",
    "    return bg_transparent\n",
    "\n",
    "\n",
    "def get_processed_inputs(image, input_points):\n",
    "    \n",
    "    # Use the processor to generate the right inputs for SAM\n",
    "    # Use \"image\" as your image\n",
    "    # Use 'input_points' as your input_points,\n",
    "    # and remember to use the option return_tensors='pt'\n",
    "    # Also, remember to add .to(\"cuda\") at the end\n",
    "\n",
    "\n",
    "    inputs = processor(image, input_points=input_points, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    pred_masks = outputs.pred_masks.cpu()\n",
    "    original_sizes = inputs[\"original_sizes\"].cpu()\n",
    "    reshaped_input_sizes = inputs[\"reshaped_input_sizes\"].cpu()\n",
    "\n",
    "    print(f\"pred_masks shape: {pred_masks.shape}\")\n",
    "    print(f\"original_sizes shape: {original_sizes.shape}\")\n",
    "    print(f\"reshaped_input_sizes shape: {reshaped_input_sizes.shape}\")\n",
    "\n",
    "    \n",
    "    # Now let's post process the outputs of SAM to obtain the masks\n",
    "    masks = processor.image_processor.post_process_masks(pred_masks, original_sizes, reshaped_input_sizes)\n",
    "    \n",
    "    # Here we select the mask with the highest score\n",
    "    # as the mask we will use. You can experiment with also\n",
    "    # other selection criteria, for example the largest mask\n",
    "    # instead of the most confident mask\n",
    "    best_mask = masks[0][0][outputs.iou_scores.argmax()] \n",
    "\n",
    "    # NOTE: we invert the mask by using the ~ operator because\n",
    "    # so that the subject pixels will have a value of 0 and the\n",
    "    # background pixels a value of 1. This will make it more convenient\n",
    "    # to infill the background\n",
    "    return ~best_mask.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the image and resize it to 512x512 pixels\n",
    "raw_image = Image.open(\"car.png\").convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# define a few points on the image (that indicate where the car is)\n",
    "input_points = [[[150, 170], [300, 250]]]\n",
    "\n",
    "# Create a drawing object\n",
    "image_with_stars = raw_image.copy()\n",
    "draw = ImageDraw.Draw(image_with_stars)\n",
    "\n",
    "# Draw each point on the image\n",
    "for point in input_points[0]:\n",
    "    x, y = point\n",
    "    radius = 5  # Marker size\n",
    "    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=\"red\", outline=\"black\")\n",
    "\n",
    "\n",
    "# generate a mask using SAM\n",
    "mask = get_processed_inputs(raw_image, input_points)\n",
    "\n",
    "# visualize the mask\n",
    "mask_image = Image.fromarray(mask_to_rgb(mask)).resize((128, 128))\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(image_with_stars)\n",
    "ax2.imshow(mask_image)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AutoPipelineForInpainting pipeline \n",
    "# (remember the diffusers demo in lesson 5)\n",
    "# The checkpoint we want to use is \n",
    "# \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\"\n",
    "# Remember to add torch_dtype=torch.float16 as an option\n",
    "\n",
    "model_name = \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\"\n",
    "pipeline = AutoPipelineForInpainting.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "# This will make it more efficient on our hardware\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint(raw_image, input_mask, prompt, negative_prompt=None, seed=74294536, cfgs=7):\n",
    "    \n",
    "    mask_image = Image.fromarray(input_mask)\n",
    "    rand_gen = torch.manual_seed(seed)\n",
    "    \n",
    "    image = pipeline(\n",
    "        prompt=prompt, \n",
    "        negative_prompt=negative_prompt, \n",
    "        image=raw_image, \n",
    "        mask_image=mask_image, \n",
    "        generator=rand_gen, \n",
    "        guidance_scale=cfgs\n",
    "    ).images[0]\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a car driving on Mars. Studio lights, 1970s\"\n",
    "negative_prompt = \"artifacts, low quality, distortion\"\n",
    "\n",
    "image = inpaint(raw_image, mask, prompt, negative_prompt)\n",
    "\n",
    "fig = make_image_grid([raw_image, Image.fromarray(mask_to_rgb(mask)), image.resize((512, 512))], rows=1, cols=3)\n",
    "\n",
    "plt.imshow(fig)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://78713d80a3c3a632c3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://78713d80a3c3a632c3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://78713d80a3c3a632c3.gradio.live\n"
     ]
    }
   ],
   "source": [
    "my_app = app.generate_app(get_processed_inputs, inpaint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "my_app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_file = \"monalisa.png\"\n",
    "# mask_file = \"monalisa_mask.png\"\n",
    "\n",
    "# prompt = \"oil painting of a woman, sfumato, renaissance, low details, Da Vinci\"\n",
    "# negative_prompt = \"bad anatomy, deformed, ugly, disfigured\"\n",
    "# guidance_scale = 1.5\n",
    "# rand_gen = torch.manual_seed(74294536)\n",
    "\n",
    "# init_image = load_image(image_file).resize((512, 512))\n",
    "# mask_image = load_image(mask_file).resize((512, 512))\n",
    "\n",
    "# image = pipeline(\n",
    "#     prompt=prompt, \n",
    "#     negative_prompt=negative_prompt, \n",
    "#     image=init_image, \n",
    "#     mask_image=mask_image, \n",
    "#     generator=rand_gen, \n",
    "#     guidance_scale=guidance_scale\n",
    "# ).images[0]\n",
    "\n",
    "\n",
    "# fig = make_image_grid([init_image, mask_image, image], rows=1, cols=3)\n",
    "# fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
